{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3\n",
    "\n",
    "You are not required to use QuickPOMDPs, but the examples will use it.\n",
    "\n",
    "The following code shows a definition of a problem with 4 nonterminal states. It receives a reward of 3 in state 1 and then terminates immediately (state 5 is a \"terminal state\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "using QuickPOMDPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = 1:5\n",
    "\n",
    "A = [-1, 1] # -1 is left, 1 is right\n",
    "\n",
    "function T(s, a, sp) # returns probability of transitioning to sp given s, a\n",
    "    # handle transitioning to the terminal state\n",
    "    if s == 1\n",
    "        if sp == 5\n",
    "            return 1.0\n",
    "        else\n",
    "            return 0.0\n",
    "        end\n",
    "    # now handle normal transitions\n",
    "    elseif sp == clamp(s + a, 1, 4)\n",
    "        return 0.9\n",
    "    elseif sp == clamp(s - a, 1, 4)\n",
    "        return 0.1\n",
    "    else\n",
    "        return 0.0\n",
    "    end\n",
    "end\n",
    "\n",
    "function R(s, a)\n",
    "    if s == 1\n",
    "        return 3.0\n",
    "    else\n",
    "        return 0.0\n",
    "    end\n",
    "end\n",
    "\n",
    "γ = 0.99\n",
    "\n",
    "terminals = Set(5) # set of terminal states - no reward and no transitioning out of these states\n",
    "\n",
    "m = DiscreteExplicitMDP(S, A, T, R, γ, terminals=terminals);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the homework problem, you will have to account for the key. You may want to continue using `Int`s to represent the state, or you can use something like more complex like [`NamedTuple`s](https://docs.julialang.org/en/v1/base/base/#Core.NamedTuple)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you get ready to solve the problem, you may wish to use the [DiscreteValueIteration package](https://github.com/JuliaPOMDP/DiscreteValueIteration.jl). It will be able to solve the problem. For this question, see especially the [`POMDPs.value` function](https://juliapomdp.github.io/POMDPs.jl/stable/api/#POMDPs.value)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 4\n",
    "\n",
    "In this question, you need only define the generative model for the continuous-state MDP.\n",
    "\n",
    "The following MDP has a generative model defined for it, but the next state is simply the action plus a uniformly-generated random number between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "using POMDPs\n",
    "using QuickPOMDPs\n",
    "using Distributions\n",
    "using POMDPPolicies\n",
    "using POMDPSimulators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = QuickMDP(\n",
    "    function G(s, a, rng) # this is the generative model - it takes in a state, action, and random number generator\n",
    "        sp = a + rand(rng) # the next state, s'\n",
    "        r = -abs(s)+0.8*abs(a) # strange reward function\n",
    "        return (sp=sp, r=r) # package state and reward in a NamedTuple to return\n",
    "    end,\n",
    "    initialstate_distribution = Normal(), # a distribution from Distributions.jl to draw initial states from\n",
    "    actiontype = Float64 # since there is no other way to infer the type of the actions, we have to tell it\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To define a policy, define a function that takes in a state and returns the action, then wrap that in a FunctionPolicy from POMDPPolicies.jl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FunctionPolicy{typeof(pfunc)}(pfunc)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function pfunc(s)\n",
    "    return -s\n",
    "end\n",
    "policy = FunctionPolicy(pfunc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run a simulation and get a reward, use the rollout simulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-19.362495060704443"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim = RolloutSimulator(max_steps=100)\n",
    "r = simulate(sim, m, policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With many such simulations, you can evaluate different policies.\n",
    "\n",
    "If simulations are going too slowly, ask on Piazza or consult the [Performance Tips](https://docs.julialang.org/en/v1/manual/performance-tips/#Avoid-global-variables-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 5\n",
    "\n",
    "I won't give too many hints on this one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "using DMUStudent\n",
    "using DMUStudent.HW2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 1\n",
    "m = UnresponsiveACASMDP(n); # this is the most coarsly discretized version of the problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dict{Int64,Array{Float64,2}} with 3 entries:\n",
       "  2 => [0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 1.0 0.0; 0.0 0.0 … …\n",
       "  3 => [0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 1.0 0.0; 0.0 0.0 … …\n",
       "  1 => [0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 1.0 0.0; 0.0 0.0 … …"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = transition_matrices(m) # a dictionary of transition matrices for each action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t[1][2,3] # probability of transitioning from 2 to 3 with action 1, i.e. T(3|2,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dict{Int64,Array{Float64,1}} with 3 entries:\n",
       "  2 => [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0…\n",
       "  3 => [-1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0  …  0.0, 0.0…\n",
       "  1 => [-1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0  …  0.0, 0.0…"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = reward_vectors(m) # a dictionary of reward vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r[1][2] # reward collected in state 2 if action 1 is taken, i.e. R(2,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1250"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "length(r[1]) # number of states is given by the size of the vectors. Make sure your value vector has this size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 2 # by increasing n, we have a finer discretization\n",
    "m2 = UnresponsiveACASMDP(n)\n",
    "r2 = reward_vectors(m2)\n",
    "length(r2[1]) # this one has many more states!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you run into memory issues, see the docstring for `transition_matrices`. If code in the problem definition is running too slow, contact the instructor; we may be able to speed it up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation complete! Score: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Warning: Incorrect value for state 9001\n",
      "└ @ DMUStudent.HW2 none:21\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to evaluate, use\n",
    "v = zeros(length(r2[1])) # this should be your actual value function\n",
    "evaluate(v, \"hw2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "ErrorException",
     "evalue": "You must use your indentikey@colorado.edu email address. Your identikey is four letters followed by four numbers.",
     "output_type": "error",
     "traceback": [
      "You must use your indentikey@colorado.edu email address. Your identikey is four letters followed by four numbers.",
      "",
      "Stacktrace:",
      " [1] error(::String) at ./error.jl:33",
      " [2] #submit#1(::String, ::typeof(submit), ::Array{Float64,1}, ::String, ::String) at /home/zach/.julia/packages/DMUStudent/KHV8b/src/DMUStudent.jl:77",
      " [3] (::DMUStudent.var\"#kw##submit\")(::NamedTuple{(:nickname,),Tuple{String}}, ::typeof(submit), ::Array{Float64,1}, ::String, ::String) at ./none:0",
      " [4] top-level scope at In[36]:1"
     ]
    }
   ],
   "source": [
    "# and to submit, use\n",
    "submit(v, \"hw2\", \"identikey@colorado.edu\", nickname=\"nickname\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.3.1",
   "language": "julia",
   "name": "julia-1.3"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.3.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
